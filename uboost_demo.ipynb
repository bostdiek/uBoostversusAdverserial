{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Boosting-to-Uniformity\" data-toc-modified-id=\"Boosting-to-Uniformity-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Boosting to Uniformity</a></div><div class=\"lev2 toc-item\"><a href=\"#Distribution-of-events-in-different-files-in-the-Dalitz-features\" data-toc-modified-id=\"Distribution-of-events-in-different-files-in-the-Dalitz-features-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Distribution of events in different files in the Dalitz features</a></div><div class=\"lev2 toc-item\"><a href=\"#Preparation-of-train/test-datasets\" data-toc-modified-id=\"Preparation-of-train/test-datasets-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Preparation of train/test datasets</a></div><div class=\"lev2 toc-item\"><a href=\"#Setting-up-classifiers,-training\" data-toc-modified-id=\"Setting-up-classifiers,-training-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Setting up classifiers, training</a></div><div class=\"lev2 toc-item\"><a href=\"#Lets-look-at-the-results-of-training\" data-toc-modified-id=\"Lets-look-at-the-results-of-training-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Lets look at the results of training</a></div><div class=\"lev2 toc-item\"><a href=\"#SDE-(squared-deviation-of-efficiency)-learning-curve\" data-toc-modified-id=\"SDE-(squared-deviation-of-efficiency)-learning-curve-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>SDE (squared deviation of efficiency) learning curve</a></div><div class=\"lev1 toc-item\"><a href=\"#ROC-curves-after-training\" data-toc-modified-id=\"ROC-curves-after-training-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>ROC curves after training</a></div><div class=\"lev2 toc-item\"><a href=\"#Signal-efficiency\" data-toc-modified-id=\"Signal-efficiency-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Signal efficiency</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting to Uniformity\n",
    "\n",
    "In physical applications frequently we need to achieve uniformity of predictions along some features.\n",
    "For instance, when testing the existence of new particle, we need classifier to be uniform in background along the mass (otherwise one can get false discovery due to peaking background).\n",
    "\n",
    "This notebook contains some comparison of classifiers. The target is to obtain flat effiency in __signal__ (without significally loosing quality of classification) in Dalitz features.\n",
    "\n",
    "The classifiers compared are \n",
    "\n",
    "* plain __GradientBoosting__ \n",
    "* __uBoost__\n",
    "* gradient boosting with knn-Ada loss (__UGB+knnAda__) \n",
    "* gradient boosting with FlatnessLoss (__UGB+FlatnessLoss__)\n",
    "\n",
    "We use dataset from paper about `uBoost` for demonstration purposes.\n",
    "We have plenty of data here, so results are quite stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: wget: command not found\r\n"
     ]
    }
   ],
   "source": [
    "# downloading data\n",
    "!wget -O ../data/dalitzdata.root -nc https://github.com/arogozhnikov/hep_ml/blob/data/data_to_download/dalitzdata.root?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryan/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named rep.estimators",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4f2c2480f4ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# this wrapper makes it possible to train on subset of features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSklearnClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhep_ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommonutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named rep.estimators"
     ]
    }
   ],
   "source": [
    "import pandas, numpy\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# this wrapper makes it possible to train on subset of features\n",
    "from rep.estimators import SklearnClassifier\n",
    "\n",
    "from hep_ml.commonutils import train_test_split\n",
    "from hep_ml import uboost, gradientboosting as ugb, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import root_numpy\n",
    "used_columns = [\"Y1\", \"Y2\", \"Y3\", \"M2AB\", \"M2AC\"]\n",
    "data = pandas.DataFrame(root_numpy.root2array('../data/dalitzdata.root', treename='tree'))\n",
    "labels = data['labels']\n",
    "data = data.drop('labels', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of events in different files in the Dalitz features\n",
    "As we can see, the background is distributed mostly in the corners of Dalitz plot, <br />\n",
    "and for traditional classifiers this results in poor effieciency of signal in the corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_distribution(data_frame, var_name1='M2AB', var_name2='M2AC', bins=40):\n",
    "    \"\"\"The function to plot 2D distribution histograms\"\"\"\n",
    "    pylab.hist2d(data_frame[var_name1], data_frame[var_name2], bins = 40, cmap=cm.Blues)\n",
    "    pylab.xlabel(var_name1)\n",
    "    pylab.ylabel(var_name2)\n",
    "    pylab.colorbar()\n",
    "\n",
    "pylab.figure(figsize=(12, 6))\n",
    "subplot(1, 2, 1), pylab.title(\"signal\"),       plot_distribution(data[labels==1])\n",
    "subplot(1, 2, 2), pylab.title(\"background\"),   plot_distribution(data[labels==0])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(data, labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up classifiers, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uniform_features  = [\"M2AB\", \"M2AC\"]\n",
    "train_features = [\"Y1\", \"Y2\", \"Y3\"]\n",
    "n_estimators = 150\n",
    "base_estimator = DecisionTreeClassifier(max_depth=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__uBoost__ training takes much time, so we reduce number of efficiency_steps, use prediction smoothing and run uBoost in threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rep.metaml import ClassifiersFactory\n",
    "\n",
    "classifiers = ClassifiersFactory()\n",
    "\n",
    "base_ada = GradientBoostingClassifier(max_depth=4, n_estimators=n_estimators, learning_rate=0.1)\n",
    "classifiers['AdaBoost'] = SklearnClassifier(base_ada, features=train_features)\n",
    "\n",
    "\n",
    "knnloss = ugb.KnnAdaLossFunction(uniform_features, knn=10, uniform_label=1)\n",
    "ugbKnn = ugb.UGradientBoostingClassifier(loss=knnloss, max_depth=4, n_estimators=n_estimators,\n",
    "                                        learning_rate=0.4, train_features=train_features)\n",
    "classifiers['uGB+knnAda'] = SklearnClassifier(ugbKnn) \n",
    "\n",
    "uboost_clf = uboost.uBoostClassifier(uniform_features=uniform_features, uniform_label=1,\n",
    "                                     base_estimator=base_estimator, \n",
    "                                     n_estimators=n_estimators, train_features=train_features, \n",
    "                                     efficiency_steps=12, n_threads=4)\n",
    "classifiers['uBoost'] = SklearnClassifier(uboost_clf)\n",
    "\n",
    "flatnessloss = ugb.KnnFlatnessLossFunction(uniform_features, fl_coefficient=3., power=1.3, uniform_label=1)\n",
    "ugbFL = ugb.UGradientBoostingClassifier(loss=flatnessloss, max_depth=4, \n",
    "                                       n_estimators=n_estimators, \n",
    "                                       learning_rate=0.1, train_features=train_features)\n",
    "classifiers['uGB+FL'] = SklearnClassifier(ugbFL)\n",
    "\n",
    "\n",
    "classifiers.fit(trainX, trainY, parallel_profile='threads-4')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets look at the results of training\n",
    "dependence of quality on the number of trees built (ROC AUC - area under the ROC curve, the more the better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rep.report.metrics import RocAuc\n",
    "report = classifiers.test_on(testX, testY)\n",
    "\n",
    "ylim(0.88, 0.94)\n",
    "report.learning_curve(RocAuc(), steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDE (squared deviation of efficiency) learning curve\n",
    "SDE vs the number of built trees. SDE is memtric of nonuniformity - less is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hep_ml.metrics import BinBasedSDE, KnnBasedCvM\n",
    "report.learning_curve(BinBasedSDE(uniform_features, uniform_label=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##CvM learning curve\n",
    "CvM is metric of non-uniformity based on Cramer-von Mises distance. We are using Knn version now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "report.learning_curve(KnnBasedCvM(uniform_features, uniform_label=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curves after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "report.roc().plot(new_plot=True, figsize=[10, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal efficiency \n",
    "global cut corresponds to average signal efficiency=0.5. In ideal case the picture shall be white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "report.efficiencies_2d(uniform_features, efficiency=0.5, signal_label=1, n_bins=15, \n",
    "                       labels_dict={1: 'signal'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same for global efficiency = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "report.efficiencies_2d(uniform_features, efficiency=0.7, signal_label=1, n_bins=15, \n",
    "                       labels_dict={1: 'signal'})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "156px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "40px",
    "left": "559px",
    "right": "14px",
    "top": "0px",
    "width": "223px"
   },
   "toc_section_display": "none",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
